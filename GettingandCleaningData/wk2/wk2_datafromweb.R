##===============================================  
##  GETTING DATA FROM WEB PAGES INTO R - wk 2
##===============================================  
##  

##  Reading HTML ---
##  
con = url ("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines (con)
close (con)
htmlCode

##  Parsing with XML ---
##  
install.packages ("XML")
library (XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse (url, useInternalNodes = TRUE)

xpathSApply (html, "//title", xmlValue)

##  Parsing a GET from the httr package ---
##  
install.packages ("httr")
library (httr); html2 = GET (url)
content2 = content (html2, as = "text")
parsedHtml = htmlParse (content2, asText = TRUE)
xpathSApply (parsedHtml, "//title", xmlValue)

##  Accessing websites with passwords ---
##  
##  First, we'll try to access a password-required site with no auth.
##  Notice we get 401 error. ---
##  
pg1 = GET ("http://httpbin.org/basic-auth/user/passwd")
pg1

##  Now try by passing in a user name and password ---
##  
pg2 = GET (
    "http://httpbin.org/basic-auth/user/passwd",
    authenticate("user", "passwd")
)
pg2

##  Using handles ---
##  
##  Using handles allows you to keep auth information to use elsewhere in your
##  script. ---
##  
google = handle ("http://google.com")
pg1 = GET (handle = google, path = "/")
pg2 = GET (handle = google, path = "search")

